{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# from core.utils import create_image_dataset\n",
    "# num_demos = 300\n",
    "# seed = 42\n",
    "# create_image_dataset('MiniWorld-OneRoom-v0', 'top', num_demos=num_demos, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms.transforms import RandomApply\n",
    "from core.transforms import GaussianBlur, TwoCropsTransform\n",
    "from core.custom_dataset import DatasetFolderSorted, ImageFolderSorted\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from agents.net import MLP, Encoder, SimSiam\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "augmentation = [\n",
    "    transforms.RandomResizedCrop(50, scale=(0.2, 1.)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    'dataset/MiniWorld-OneRoom-v0/agent/D300',\n",
    "    TwoCropsTransform(\n",
    "        transforms.Compose(augmentation)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimSiam().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CosineSimilarity(dim=1).cuda()\n",
    "fix_pred_lr = False\n",
    "init_lr = 0.001\n",
    "\n",
    "if fix_pred_lr:\n",
    "    optim_params = [{'params': model.encoder.parameters(), 'fix_lr': False},\n",
    "                    {'params': model.predictor.parameters(), 'fix_lr': True}]\n",
    "else:\n",
    "    # optim_params = model.parameters()\n",
    "    optim_params = [{'params': model.encoder.parameters(), 'lr': 1e-4},\n",
    "                    {'params': model.predictor.parameters(), 'lr': 1e-3}]\n",
    "\n",
    "optimizer = optim.Adam(optim_params, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][ 0/78]\tTime  0.425 ( 0.425)\tData  0.163 ( 0.163)\tLoss -0.0095 (-0.0095)\n",
      "Epoch: [1][ 7/78]\tTime  0.203 ( 0.232)\tData  0.167 ( 0.164)\tLoss -0.1689 (-0.0938)\n",
      "Epoch: [1][14/78]\tTime  0.223 ( 0.219)\tData  0.182 ( 0.163)\tLoss -0.2685 (-0.1582)\n",
      "Epoch: [1][21/78]\tTime  0.207 ( 0.214)\tData  0.166 ( 0.162)\tLoss -0.3099 (-0.2041)\n",
      "Epoch: [1][28/78]\tTime  0.199 ( 0.211)\tData  0.157 ( 0.162)\tLoss -0.3573 (-0.2369)\n",
      "Epoch: [1][35/78]\tTime  0.196 ( 0.208)\tData  0.159 ( 0.161)\tLoss -0.3766 (-0.2613)\n",
      "Epoch: [1][42/78]\tTime  0.203 ( 0.206)\tData  0.160 ( 0.160)\tLoss -0.3782 (-0.2806)\n",
      "Epoch: [1][49/78]\tTime  0.216 ( 0.206)\tData  0.172 ( 0.160)\tLoss -0.3894 (-0.2957)\n",
      "Epoch: [1][56/78]\tTime  0.199 ( 0.205)\tData  0.154 ( 0.160)\tLoss -0.4000 (-0.3076)\n",
      "Epoch: [1][63/78]\tTime  0.274 ( 0.207)\tData  0.223 ( 0.161)\tLoss -0.3859 (-0.3171)\n",
      "Epoch: [1][70/78]\tTime  0.224 ( 0.207)\tData  0.177 ( 0.161)\tLoss -0.4053 (-0.3258)\n",
      "Epoch: [1][77/78]\tTime  0.110 ( 0.206)\tData  0.081 ( 0.161)\tLoss -0.4198 (-0.3329)\n",
      "Epoch: [2][ 0/78]\tTime  0.215 ( 0.215)\tData  0.171 ( 0.171)\tLoss -0.3898 (-0.3898)\n",
      "Epoch: [2][ 7/78]\tTime  0.197 ( 0.199)\tData  0.153 ( 0.157)\tLoss -0.3742 (-0.3953)\n",
      "Epoch: [2][14/78]\tTime  0.196 ( 0.199)\tData  0.152 ( 0.156)\tLoss -0.3883 (-0.3931)\n",
      "Epoch: [2][21/78]\tTime  0.196 ( 0.198)\tData  0.151 ( 0.155)\tLoss -0.3942 (-0.3929)\n",
      "Epoch: [2][28/78]\tTime  0.197 ( 0.198)\tData  0.153 ( 0.155)\tLoss -0.3956 (-0.3919)\n",
      "Epoch: [2][35/78]\tTime  0.203 ( 0.198)\tData  0.158 ( 0.155)\tLoss -0.3733 (-0.3925)\n",
      "Epoch: [2][42/78]\tTime  0.204 ( 0.199)\tData  0.159 ( 0.156)\tLoss -0.4134 (-0.3947)\n",
      "Epoch: [2][49/78]\tTime  0.202 ( 0.199)\tData  0.156 ( 0.156)\tLoss -0.4215 (-0.3979)\n",
      "Epoch: [2][56/78]\tTime  0.201 ( 0.199)\tData  0.156 ( 0.156)\tLoss -0.4255 (-0.3995)\n",
      "Epoch: [2][63/78]\tTime  0.196 ( 0.199)\tData  0.154 ( 0.156)\tLoss -0.4186 (-0.4011)\n",
      "Epoch: [2][70/78]\tTime  0.197 ( 0.199)\tData  0.152 ( 0.156)\tLoss -0.4210 (-0.4028)\n",
      "Epoch: [2][77/78]\tTime  0.103 ( 0.198)\tData  0.078 ( 0.155)\tLoss -0.4118 (-0.4044)\n",
      "Epoch: [3][ 0/78]\tTime  0.199 ( 0.199)\tData  0.154 ( 0.154)\tLoss -0.4101 (-0.4101)\n",
      "Epoch: [3][ 7/78]\tTime  0.206 ( 0.199)\tData  0.162 ( 0.156)\tLoss -0.4486 (-0.4296)\n",
      "Epoch: [3][14/78]\tTime  0.205 ( 0.200)\tData  0.162 ( 0.156)\tLoss -0.4320 (-0.4333)\n",
      "Epoch: [3][21/78]\tTime  0.199 ( 0.199)\tData  0.155 ( 0.155)\tLoss -0.4519 (-0.4380)\n",
      "Epoch: [3][28/78]\tTime  0.206 ( 0.200)\tData  0.160 ( 0.156)\tLoss -0.4584 (-0.4419)\n",
      "Epoch: [3][35/78]\tTime  0.196 ( 0.200)\tData  0.151 ( 0.156)\tLoss -0.4725 (-0.4443)\n",
      "Epoch: [3][42/78]\tTime  0.197 ( 0.200)\tData  0.155 ( 0.156)\tLoss -0.4829 (-0.4486)\n",
      "Epoch: [3][49/78]\tTime  0.202 ( 0.199)\tData  0.161 ( 0.157)\tLoss -0.4759 (-0.4517)\n",
      "Epoch: [3][56/78]\tTime  0.199 ( 0.199)\tData  0.158 ( 0.157)\tLoss -0.4916 (-0.4554)\n",
      "Epoch: [3][63/78]\tTime  0.197 ( 0.199)\tData  0.155 ( 0.156)\tLoss -0.4891 (-0.4586)\n",
      "Epoch: [3][70/78]\tTime  0.192 ( 0.198)\tData  0.152 ( 0.156)\tLoss -0.4715 (-0.4616)\n",
      "Epoch: [3][77/78]\tTime  0.104 ( 0.197)\tData  0.080 ( 0.155)\tLoss -0.4969 (-0.4639)\n",
      "Epoch: [4][ 0/78]\tTime  0.196 ( 0.196)\tData  0.156 ( 0.156)\tLoss -0.5071 (-0.5071)\n",
      "Epoch: [4][ 7/78]\tTime  0.204 ( 0.199)\tData  0.162 ( 0.157)\tLoss -0.5030 (-0.5035)\n",
      "Epoch: [4][14/78]\tTime  0.198 ( 0.198)\tData  0.160 ( 0.157)\tLoss -0.4991 (-0.4985)\n",
      "Epoch: [4][21/78]\tTime  0.193 ( 0.197)\tData  0.152 ( 0.157)\tLoss -0.5247 (-0.5018)\n",
      "Epoch: [4][28/78]\tTime  0.191 ( 0.197)\tData  0.152 ( 0.157)\tLoss -0.5211 (-0.5038)\n",
      "Epoch: [4][35/78]\tTime  0.193 ( 0.197)\tData  0.152 ( 0.156)\tLoss -0.5206 (-0.5051)\n",
      "Epoch: [4][42/78]\tTime  0.190 ( 0.197)\tData  0.150 ( 0.156)\tLoss -0.5058 (-0.5071)\n",
      "Epoch: [4][49/78]\tTime  0.195 ( 0.196)\tData  0.153 ( 0.156)\tLoss -0.5254 (-0.5078)\n",
      "Epoch: [4][56/78]\tTime  0.194 ( 0.196)\tData  0.154 ( 0.156)\tLoss -0.5295 (-0.5090)\n",
      "Epoch: [4][63/78]\tTime  0.208 ( 0.197)\tData  0.165 ( 0.156)\tLoss -0.5149 (-0.5092)\n",
      "Epoch: [4][70/78]\tTime  0.196 ( 0.197)\tData  0.151 ( 0.156)\tLoss -0.5225 (-0.5094)\n",
      "Epoch: [4][77/78]\tTime  0.104 ( 0.196)\tData  0.078 ( 0.155)\tLoss -0.5423 (-0.5107)\n",
      "Epoch: [5][ 0/78]\tTime  0.208 ( 0.208)\tData  0.164 ( 0.164)\tLoss -0.4986 (-0.4986)\n",
      "Epoch: [5][ 7/78]\tTime  0.198 ( 0.199)\tData  0.157 ( 0.156)\tLoss -0.5318 (-0.5268)\n",
      "Epoch: [5][14/78]\tTime  0.195 ( 0.198)\tData  0.153 ( 0.157)\tLoss -0.5439 (-0.5264)\n",
      "Epoch: [5][21/78]\tTime  0.193 ( 0.197)\tData  0.151 ( 0.156)\tLoss -0.5474 (-0.5285)\n",
      "Epoch: [5][28/78]\tTime  0.188 ( 0.196)\tData  0.147 ( 0.155)\tLoss -0.5477 (-0.5315)\n",
      "Epoch: [5][35/78]\tTime  0.200 ( 0.198)\tData  0.157 ( 0.156)\tLoss -0.5305 (-0.5317)\n",
      "Epoch: [5][42/78]\tTime  0.196 ( 0.197)\tData  0.154 ( 0.156)\tLoss -0.5252 (-0.5321)\n",
      "Epoch: [5][49/78]\tTime  0.194 ( 0.197)\tData  0.153 ( 0.156)\tLoss -0.5387 (-0.5348)\n",
      "Epoch: [5][56/78]\tTime  0.193 ( 0.197)\tData  0.154 ( 0.155)\tLoss -0.5150 (-0.5354)\n",
      "Epoch: [5][63/78]\tTime  0.198 ( 0.196)\tData  0.157 ( 0.155)\tLoss -0.5362 (-0.5363)\n",
      "Epoch: [5][70/78]\tTime  0.199 ( 0.197)\tData  0.157 ( 0.155)\tLoss -0.5466 (-0.5362)\n",
      "Epoch: [5][77/78]\tTime  0.105 ( 0.195)\tData  0.079 ( 0.154)\tLoss -0.5224 (-0.5375)\n",
      "Epoch: [6][ 0/78]\tTime  0.249 ( 0.249)\tData  0.177 ( 0.177)\tLoss -0.5451 (-0.5451)\n",
      "Epoch: [6][ 7/78]\tTime  0.190 ( 0.203)\tData  0.149 ( 0.158)\tLoss -0.5689 (-0.5531)\n",
      "Epoch: [6][14/78]\tTime  0.199 ( 0.203)\tData  0.158 ( 0.157)\tLoss -0.5630 (-0.5496)\n",
      "Epoch: [6][21/78]\tTime  0.229 ( 0.210)\tData  0.164 ( 0.160)\tLoss -0.5630 (-0.5487)\n",
      "Epoch: [6][28/78]\tTime  0.200 ( 0.211)\tData  0.160 ( 0.161)\tLoss -0.5651 (-0.5509)\n",
      "Epoch: [6][35/78]\tTime  0.194 ( 0.208)\tData  0.152 ( 0.160)\tLoss -0.5656 (-0.5501)\n",
      "Epoch: [6][42/78]\tTime  0.188 ( 0.206)\tData  0.148 ( 0.158)\tLoss -0.5315 (-0.5504)\n",
      "Epoch: [6][49/78]\tTime  0.200 ( 0.206)\tData  0.158 ( 0.159)\tLoss -0.5567 (-0.5501)\n",
      "Epoch: [6][56/78]\tTime  0.223 ( 0.208)\tData  0.160 ( 0.160)\tLoss -0.5644 (-0.5498)\n",
      "Epoch: [6][63/78]\tTime  0.191 ( 0.207)\tData  0.152 ( 0.160)\tLoss -0.5597 (-0.5497)\n",
      "Epoch: [6][70/78]\tTime  0.186 ( 0.206)\tData  0.150 ( 0.159)\tLoss -0.5419 (-0.5503)\n",
      "Epoch: [6][77/78]\tTime  0.108 ( 0.204)\tData  0.084 ( 0.158)\tLoss -0.5453 (-0.5509)\n",
      "Epoch: [7][ 0/78]\tTime  0.191 ( 0.191)\tData  0.151 ( 0.151)\tLoss -0.5401 (-0.5401)\n",
      "Epoch: [7][ 7/78]\tTime  0.186 ( 0.192)\tData  0.149 ( 0.153)\tLoss -0.5632 (-0.5552)\n",
      "Epoch: [7][14/78]\tTime  0.196 ( 0.193)\tData  0.156 ( 0.153)\tLoss -0.5574 (-0.5574)\n",
      "Epoch: [7][21/78]\tTime  0.193 ( 0.194)\tData  0.156 ( 0.154)\tLoss -0.5491 (-0.5547)\n",
      "Epoch: [7][28/78]\tTime  0.196 ( 0.193)\tData  0.155 ( 0.154)\tLoss -0.5604 (-0.5574)\n",
      "Epoch: [7][35/78]\tTime  0.191 ( 0.193)\tData  0.149 ( 0.154)\tLoss -0.5700 (-0.5586)\n",
      "Epoch: [7][42/78]\tTime  0.194 ( 0.193)\tData  0.153 ( 0.154)\tLoss -0.5555 (-0.5585)\n",
      "Epoch: [7][49/78]\tTime  0.194 ( 0.193)\tData  0.154 ( 0.154)\tLoss -0.5741 (-0.5587)\n",
      "Epoch: [7][56/78]\tTime  0.203 ( 0.194)\tData  0.162 ( 0.154)\tLoss -0.5299 (-0.5591)\n",
      "Epoch: [7][63/78]\tTime  0.195 ( 0.194)\tData  0.154 ( 0.154)\tLoss -0.5584 (-0.5590)\n",
      "Epoch: [7][70/78]\tTime  0.190 ( 0.194)\tData  0.150 ( 0.154)\tLoss -0.5732 (-0.5594)\n",
      "Epoch: [7][77/78]\tTime  0.105 ( 0.193)\tData  0.080 ( 0.153)\tLoss -0.5236 (-0.5591)\n",
      "Epoch: [8][ 0/78]\tTime  0.193 ( 0.193)\tData  0.153 ( 0.153)\tLoss -0.5437 (-0.5437)\n",
      "Epoch: [8][ 7/78]\tTime  0.181 ( 0.191)\tData  0.144 ( 0.152)\tLoss -0.5612 (-0.5623)\n",
      "Epoch: [8][14/78]\tTime  0.196 ( 0.192)\tData  0.155 ( 0.152)\tLoss -0.5380 (-0.5589)\n",
      "Epoch: [8][21/78]\tTime  0.187 ( 0.192)\tData  0.151 ( 0.152)\tLoss -0.5583 (-0.5564)\n",
      "Epoch: [8][28/78]\tTime  0.194 ( 0.191)\tData  0.154 ( 0.152)\tLoss -0.5569 (-0.5575)\n",
      "Epoch: [8][35/78]\tTime  0.194 ( 0.192)\tData  0.154 ( 0.152)\tLoss -0.5633 (-0.5587)\n",
      "Epoch: [8][42/78]\tTime  0.191 ( 0.192)\tData  0.151 ( 0.152)\tLoss -0.5484 (-0.5589)\n",
      "Epoch: [8][49/78]\tTime  0.193 ( 0.192)\tData  0.153 ( 0.153)\tLoss -0.5908 (-0.5609)\n",
      "Epoch: [8][56/78]\tTime  0.198 ( 0.193)\tData  0.158 ( 0.153)\tLoss -0.5574 (-0.5619)\n",
      "Epoch: [8][63/78]\tTime  0.194 ( 0.193)\tData  0.153 ( 0.153)\tLoss -0.5814 (-0.5627)\n",
      "Epoch: [8][70/78]\tTime  0.188 ( 0.193)\tData  0.151 ( 0.153)\tLoss -0.5708 (-0.5639)\n",
      "Epoch: [8][77/78]\tTime  0.105 ( 0.192)\tData  0.081 ( 0.152)\tLoss -0.5560 (-0.5646)\n",
      "Epoch: [9][ 0/78]\tTime  0.191 ( 0.191)\tData  0.150 ( 0.150)\tLoss -0.5657 (-0.5657)\n",
      "Epoch: [9][ 7/78]\tTime  0.190 ( 0.191)\tData  0.153 ( 0.152)\tLoss -0.5460 (-0.5532)\n",
      "Epoch: [9][14/78]\tTime  0.199 ( 0.196)\tData  0.159 ( 0.156)\tLoss -0.5486 (-0.5569)\n",
      "Epoch: [9][21/78]\tTime  0.188 ( 0.195)\tData  0.151 ( 0.155)\tLoss -0.5441 (-0.5603)\n",
      "Epoch: [9][28/78]\tTime  0.191 ( 0.194)\tData  0.150 ( 0.155)\tLoss -0.5600 (-0.5608)\n",
      "Epoch: [9][35/78]\tTime  0.198 ( 0.195)\tData  0.157 ( 0.155)\tLoss -0.5714 (-0.5627)\n",
      "Epoch: [9][42/78]\tTime  0.197 ( 0.194)\tData  0.157 ( 0.155)\tLoss -0.5815 (-0.5640)\n",
      "Epoch: [9][49/78]\tTime  0.194 ( 0.194)\tData  0.153 ( 0.154)\tLoss -0.5695 (-0.5638)\n",
      "Epoch: [9][56/78]\tTime  0.195 ( 0.194)\tData  0.155 ( 0.154)\tLoss -0.5726 (-0.5642)\n",
      "Epoch: [9][63/78]\tTime  0.196 ( 0.194)\tData  0.156 ( 0.154)\tLoss -0.5647 (-0.5655)\n",
      "Epoch: [9][70/78]\tTime  0.199 ( 0.194)\tData  0.158 ( 0.154)\tLoss -0.5547 (-0.5660)\n",
      "Epoch: [9][77/78]\tTime  0.107 ( 0.193)\tData  0.082 ( 0.153)\tLoss -0.5853 (-0.5667)\n",
      "Epoch: [10][ 0/78]\tTime  0.199 ( 0.199)\tData  0.158 ( 0.158)\tLoss -0.5845 (-0.5845)\n",
      "Epoch: [10][ 7/78]\tTime  0.189 ( 0.191)\tData  0.153 ( 0.152)\tLoss -0.5611 (-0.5736)\n",
      "Epoch: [10][14/78]\tTime  0.199 ( 0.193)\tData  0.157 ( 0.153)\tLoss -0.5551 (-0.5677)\n",
      "Epoch: [10][21/78]\tTime  0.189 ( 0.193)\tData  0.152 ( 0.153)\tLoss -0.5666 (-0.5694)\n",
      "Epoch: [10][28/78]\tTime  0.195 ( 0.193)\tData  0.155 ( 0.153)\tLoss -0.5601 (-0.5691)\n",
      "Epoch: [10][35/78]\tTime  0.196 ( 0.193)\tData  0.155 ( 0.153)\tLoss -0.5884 (-0.5715)\n",
      "Epoch: [10][42/78]\tTime  0.192 ( 0.192)\tData  0.151 ( 0.153)\tLoss -0.5510 (-0.5727)\n",
      "Epoch: [10][49/78]\tTime  0.199 ( 0.193)\tData  0.157 ( 0.153)\tLoss -0.5687 (-0.5728)\n",
      "Epoch: [10][56/78]\tTime  0.222 ( 0.193)\tData  0.176 ( 0.153)\tLoss -0.5756 (-0.5738)\n",
      "Epoch: [10][63/78]\tTime  0.188 ( 0.193)\tData  0.148 ( 0.153)\tLoss -0.5516 (-0.5732)\n",
      "Epoch: [10][70/78]\tTime  0.185 ( 0.193)\tData  0.149 ( 0.153)\tLoss -0.5695 (-0.5734)\n",
      "Epoch: [10][77/78]\tTime  0.101 ( 0.192)\tData  0.076 ( 0.152)\tLoss -0.5959 (-0.5736)\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # if args.gpu is not None:\n",
    "        #     images[0] = images[0].cuda(args.gpu, non_blocking=True)\n",
    "        #     images[1] = images[1].cuda(args.gpu, non_blocking=True)\n",
    "\n",
    "        # compute output and loss\n",
    "        # print(len(images))\n",
    "        # input()\n",
    "        p1, p2, z1, z2 = model(x1=images[0].cuda(), x2=images[1].cuda())\n",
    "        # print(p1.size(), p2.size(), z1.size(), z2.size())\n",
    "        loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "\n",
    "        losses.update(loss.item(), images[0].size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % (max(len(train_loader)//10, 1)) == 0:\n",
    "            progress.display(i)\n",
    "for epoch in range(1, 1 + 10):\n",
    "    train(train_loader, model, criterion, optimizer, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.hist_dataset import DemoDataPreviousAction\n",
    "\n",
    "cls_train_dataset = DemoDataPreviousAction(demo_folder='demos/MiniWorld-OneRoom-v0/agent', nb_demos=300, transform=transforms.Compose(augmentation))\n",
    "cls_train_dataloader = DataLoader(cls_train_dataset,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][ 0/78]\tTime  0.084 ( 0.084)\tData  0.065 ( 0.065)\tLoss 2.0715 (2.0715)\n",
      "Epoch: [1][ 7/78]\tTime  0.083 ( 0.083)\tData  0.062 ( 0.064)\tLoss 1.0289 (1.4329)\n",
      "Epoch: [1][14/78]\tTime  0.084 ( 0.083)\tData  0.063 ( 0.063)\tLoss 0.9060 (1.2427)\n",
      "Epoch: [1][21/78]\tTime  0.088 ( 0.084)\tData  0.067 ( 0.064)\tLoss 0.8038 (1.1093)\n",
      "Epoch: [1][28/78]\tTime  0.088 ( 0.085)\tData  0.068 ( 0.064)\tLoss 0.7727 (1.0317)\n",
      "Epoch: [1][35/78]\tTime  0.086 ( 0.085)\tData  0.066 ( 0.064)\tLoss 0.7756 (0.9675)\n",
      "Epoch: [1][42/78]\tTime  0.081 ( 0.084)\tData  0.062 ( 0.064)\tLoss 0.5486 (0.9156)\n",
      "Epoch: [1][49/78]\tTime  0.081 ( 0.084)\tData  0.061 ( 0.064)\tLoss 0.8673 (0.8866)\n",
      "Epoch: [1][56/78]\tTime  0.084 ( 0.084)\tData  0.064 ( 0.064)\tLoss 0.7093 (0.8592)\n",
      "Epoch: [1][63/78]\tTime  0.082 ( 0.084)\tData  0.063 ( 0.064)\tLoss 0.7399 (0.8374)\n",
      "Epoch: [1][70/78]\tTime  0.079 ( 0.084)\tData  0.062 ( 0.064)\tLoss 0.6345 (0.8213)\n",
      "Epoch: [1][77/78]\tTime  0.042 ( 0.083)\tData  0.031 ( 0.063)\tLoss 0.7244 (0.8067)\n",
      "Epoch: [2][ 0/78]\tTime  0.081 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.6320 (0.6320)\n",
      "Epoch: [2][ 7/78]\tTime  0.083 ( 0.082)\tData  0.062 ( 0.062)\tLoss 0.5815 (0.6752)\n",
      "Epoch: [2][14/78]\tTime  0.082 ( 0.082)\tData  0.062 ( 0.062)\tLoss 0.6723 (0.6835)\n",
      "Epoch: [2][21/78]\tTime  0.082 ( 0.082)\tData  0.061 ( 0.062)\tLoss 0.8096 (0.6601)\n",
      "Epoch: [2][28/78]\tTime  0.078 ( 0.082)\tData  0.061 ( 0.062)\tLoss 0.4782 (0.6488)\n",
      "Epoch: [2][35/78]\tTime  0.078 ( 0.083)\tData  0.061 ( 0.063)\tLoss 0.6622 (0.6393)\n",
      "Epoch: [2][42/78]\tTime  0.080 ( 0.083)\tData  0.060 ( 0.063)\tLoss 0.5453 (0.6274)\n",
      "Epoch: [2][49/78]\tTime  0.083 ( 0.083)\tData  0.062 ( 0.063)\tLoss 0.6682 (0.6236)\n",
      "Epoch: [2][56/78]\tTime  0.084 ( 0.083)\tData  0.065 ( 0.063)\tLoss 0.6630 (0.6222)\n",
      "Epoch: [2][63/78]\tTime  0.078 ( 0.083)\tData  0.060 ( 0.063)\tLoss 0.5744 (0.6185)\n",
      "Epoch: [2][70/78]\tTime  0.083 ( 0.083)\tData  0.066 ( 0.063)\tLoss 0.5463 (0.6134)\n",
      "Epoch: [2][77/78]\tTime  0.043 ( 0.082)\tData  0.031 ( 0.063)\tLoss 0.4656 (0.6082)\n",
      "Epoch: [3][ 0/78]\tTime  0.082 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.6251 (0.6251)\n",
      "Epoch: [3][ 7/78]\tTime  0.080 ( 0.082)\tData  0.060 ( 0.062)\tLoss 0.4511 (0.5407)\n",
      "Epoch: [3][14/78]\tTime  0.082 ( 0.082)\tData  0.062 ( 0.062)\tLoss 0.4862 (0.5512)\n",
      "Epoch: [3][21/78]\tTime  0.082 ( 0.083)\tData  0.062 ( 0.063)\tLoss 0.4334 (0.5525)\n",
      "Epoch: [3][28/78]\tTime  0.082 ( 0.083)\tData  0.062 ( 0.063)\tLoss 0.6005 (0.5515)\n",
      "Epoch: [3][35/78]\tTime  0.086 ( 0.083)\tData  0.067 ( 0.063)\tLoss 0.5874 (0.5585)\n",
      "Epoch: [3][42/78]\tTime  0.083 ( 0.083)\tData  0.062 ( 0.063)\tLoss 0.5044 (0.5502)\n",
      "Epoch: [3][49/78]\tTime  0.081 ( 0.083)\tData  0.062 ( 0.063)\tLoss 0.5353 (0.5584)\n",
      "Epoch: [3][56/78]\tTime  0.081 ( 0.083)\tData  0.061 ( 0.063)\tLoss 0.6755 (0.5559)\n",
      "Epoch: [3][63/78]\tTime  0.079 ( 0.082)\tData  0.061 ( 0.063)\tLoss 0.6248 (0.5626)\n",
      "Epoch: [3][70/78]\tTime  0.079 ( 0.082)\tData  0.060 ( 0.063)\tLoss 0.6343 (0.5640)\n",
      "Epoch: [3][77/78]\tTime  0.043 ( 0.082)\tData  0.032 ( 0.062)\tLoss 0.5232 (0.5632)\n",
      "Epoch: [4][ 0/78]\tTime  0.082 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.8109 (0.8109)\n",
      "Epoch: [4][ 7/78]\tTime  0.080 ( 0.082)\tData  0.061 ( 0.062)\tLoss 0.5790 (0.5896)\n",
      "Epoch: [4][14/78]\tTime  0.085 ( 0.083)\tData  0.065 ( 0.063)\tLoss 0.6196 (0.5697)\n",
      "Epoch: [4][21/78]\tTime  0.079 ( 0.082)\tData  0.062 ( 0.063)\tLoss 0.6671 (0.5484)\n",
      "Epoch: [4][28/78]\tTime  0.082 ( 0.082)\tData  0.062 ( 0.063)\tLoss 0.4952 (0.5539)\n",
      "Epoch: [4][35/78]\tTime  0.080 ( 0.082)\tData  0.062 ( 0.063)\tLoss 0.4945 (0.5467)\n",
      "Epoch: [4][42/78]\tTime  0.080 ( 0.082)\tData  0.060 ( 0.063)\tLoss 0.5724 (0.5510)\n",
      "Epoch: [4][49/78]\tTime  0.080 ( 0.082)\tData  0.061 ( 0.063)\tLoss 0.5282 (0.5496)\n",
      "Epoch: [4][56/78]\tTime  0.077 ( 0.082)\tData  0.060 ( 0.063)\tLoss 0.5408 (0.5453)\n",
      "Epoch: [4][63/78]\tTime  0.080 ( 0.082)\tData  0.060 ( 0.063)\tLoss 0.5017 (0.5450)\n",
      "Epoch: [4][70/78]\tTime  0.083 ( 0.083)\tData  0.062 ( 0.063)\tLoss 0.5062 (0.5435)\n",
      "Epoch: [4][77/78]\tTime  0.047 ( 0.082)\tData  0.035 ( 0.063)\tLoss 0.4213 (0.5440)\n",
      "Epoch: [5][ 0/78]\tTime  0.087 ( 0.087)\tData  0.067 ( 0.067)\tLoss 0.5834 (0.5834)\n",
      "Epoch: [5][ 7/78]\tTime  0.088 ( 0.087)\tData  0.069 ( 0.066)\tLoss 0.4936 (0.5479)\n",
      "Epoch: [5][14/78]\tTime  0.079 ( 0.084)\tData  0.062 ( 0.065)\tLoss 0.5627 (0.5332)\n",
      "Epoch: [5][21/78]\tTime  0.079 ( 0.083)\tData  0.060 ( 0.064)\tLoss 0.6595 (0.5138)\n",
      "Epoch: [5][28/78]\tTime  0.081 ( 0.083)\tData  0.062 ( 0.064)\tLoss 0.5134 (0.5371)\n",
      "Epoch: [5][35/78]\tTime  0.080 ( 0.083)\tData  0.061 ( 0.064)\tLoss 0.5686 (0.5383)\n",
      "Epoch: [5][42/78]\tTime  0.077 ( 0.083)\tData  0.058 ( 0.064)\tLoss 0.6346 (0.5385)\n",
      "Epoch: [5][49/78]\tTime  0.080 ( 0.083)\tData  0.063 ( 0.064)\tLoss 0.5609 (0.5374)\n",
      "Epoch: [5][56/78]\tTime  0.080 ( 0.082)\tData  0.063 ( 0.064)\tLoss 0.4199 (0.5276)\n",
      "Epoch: [5][63/78]\tTime  0.081 ( 0.082)\tData  0.061 ( 0.063)\tLoss 0.5651 (0.5301)\n",
      "Epoch: [5][70/78]\tTime  0.081 ( 0.082)\tData  0.062 ( 0.063)\tLoss 0.4440 (0.5336)\n",
      "Epoch: [5][77/78]\tTime  0.041 ( 0.082)\tData  0.031 ( 0.063)\tLoss 0.6355 (0.5308)\n",
      "Epoch: [6][ 0/78]\tTime  0.082 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.5803 (0.5803)\n",
      "Epoch: [6][ 7/78]\tTime  0.084 ( 0.082)\tData  0.064 ( 0.062)\tLoss 0.6572 (0.5404)\n",
      "Epoch: [6][14/78]\tTime  0.080 ( 0.081)\tData  0.063 ( 0.063)\tLoss 0.4194 (0.4915)\n",
      "Epoch: [6][21/78]\tTime  0.082 ( 0.081)\tData  0.063 ( 0.063)\tLoss 0.4390 (0.5076)\n",
      "Epoch: [6][28/78]\tTime  0.082 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.5466 (0.5079)\n",
      "Epoch: [6][35/78]\tTime  0.081 ( 0.082)\tData  0.062 ( 0.063)\tLoss 0.4021 (0.5044)\n",
      "Epoch: [6][42/78]\tTime  0.081 ( 0.082)\tData  0.061 ( 0.063)\tLoss 0.4407 (0.4977)\n",
      "Epoch: [6][49/78]\tTime  0.080 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.4050 (0.4899)\n",
      "Epoch: [6][56/78]\tTime  0.088 ( 0.082)\tData  0.067 ( 0.063)\tLoss 0.6278 (0.4926)\n",
      "Epoch: [6][63/78]\tTime  0.085 ( 0.082)\tData  0.064 ( 0.063)\tLoss 0.5482 (0.4900)\n",
      "Epoch: [6][70/78]\tTime  0.081 ( 0.082)\tData  0.062 ( 0.064)\tLoss 0.5639 (0.4935)\n",
      "Epoch: [6][77/78]\tTime  0.045 ( 0.082)\tData  0.035 ( 0.063)\tLoss 0.3833 (0.4942)\n",
      "Epoch: [7][ 0/78]\tTime  0.080 ( 0.080)\tData  0.061 ( 0.061)\tLoss 0.5100 (0.5100)\n",
      "Epoch: [7][ 7/78]\tTime  0.079 ( 0.080)\tData  0.062 ( 0.062)\tLoss 0.5999 (0.4913)\n",
      "Epoch: [7][14/78]\tTime  0.084 ( 0.081)\tData  0.065 ( 0.063)\tLoss 0.4350 (0.4904)\n",
      "Epoch: [7][21/78]\tTime  0.082 ( 0.081)\tData  0.063 ( 0.062)\tLoss 0.5696 (0.4963)\n",
      "Epoch: [7][28/78]\tTime  0.081 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.4809 (0.4938)\n",
      "Epoch: [7][35/78]\tTime  0.080 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.3543 (0.4900)\n",
      "Epoch: [7][42/78]\tTime  0.078 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.5135 (0.4847)\n",
      "Epoch: [7][49/78]\tTime  0.079 ( 0.080)\tData  0.060 ( 0.062)\tLoss 0.4840 (0.4835)\n",
      "Epoch: [7][56/78]\tTime  0.080 ( 0.080)\tData  0.062 ( 0.062)\tLoss 0.3437 (0.4845)\n",
      "Epoch: [7][63/78]\tTime  0.081 ( 0.080)\tData  0.061 ( 0.062)\tLoss 0.5621 (0.4856)\n",
      "Epoch: [7][70/78]\tTime  0.078 ( 0.080)\tData  0.059 ( 0.062)\tLoss 0.5511 (0.4858)\n",
      "Epoch: [7][77/78]\tTime  0.043 ( 0.080)\tData  0.032 ( 0.062)\tLoss 0.5088 (0.4873)\n",
      "Epoch: [8][ 0/78]\tTime  0.080 ( 0.080)\tData  0.063 ( 0.063)\tLoss 0.5194 (0.5194)\n",
      "Epoch: [8][ 7/78]\tTime  0.079 ( 0.079)\tData  0.062 ( 0.062)\tLoss 0.5461 (0.4485)\n",
      "Epoch: [8][14/78]\tTime  0.080 ( 0.079)\tData  0.062 ( 0.062)\tLoss 0.3530 (0.4720)\n",
      "Epoch: [8][21/78]\tTime  0.081 ( 0.080)\tData  0.062 ( 0.062)\tLoss 0.4512 (0.4760)\n",
      "Epoch: [8][28/78]\tTime  0.081 ( 0.080)\tData  0.062 ( 0.062)\tLoss 0.4876 (0.4604)\n",
      "Epoch: [8][35/78]\tTime  0.080 ( 0.080)\tData  0.061 ( 0.062)\tLoss 0.4168 (0.4672)\n",
      "Epoch: [8][42/78]\tTime  0.080 ( 0.080)\tData  0.061 ( 0.062)\tLoss 0.5619 (0.4662)\n",
      "Epoch: [8][49/78]\tTime  0.081 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.3988 (0.4696)\n",
      "Epoch: [8][56/78]\tTime  0.081 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.4952 (0.4724)\n",
      "Epoch: [8][63/78]\tTime  0.081 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.4971 (0.4751)\n",
      "Epoch: [8][70/78]\tTime  0.080 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.5554 (0.4763)\n",
      "Epoch: [8][77/78]\tTime  0.043 ( 0.080)\tData  0.033 ( 0.062)\tLoss 0.3689 (0.4708)\n",
      "Epoch: [9][ 0/78]\tTime  0.079 ( 0.079)\tData  0.062 ( 0.062)\tLoss 0.4977 (0.4977)\n",
      "Epoch: [9][ 7/78]\tTime  0.080 ( 0.085)\tData  0.061 ( 0.067)\tLoss 0.3614 (0.4593)\n",
      "Epoch: [9][14/78]\tTime  0.080 ( 0.083)\tData  0.061 ( 0.065)\tLoss 0.5010 (0.4725)\n",
      "Epoch: [9][21/78]\tTime  0.081 ( 0.083)\tData  0.062 ( 0.064)\tLoss 0.4635 (0.4719)\n",
      "Epoch: [9][28/78]\tTime  0.081 ( 0.083)\tData  0.062 ( 0.064)\tLoss 0.4707 (0.4807)\n",
      "Epoch: [9][35/78]\tTime  0.081 ( 0.082)\tData  0.064 ( 0.064)\tLoss 0.5277 (0.4850)\n",
      "Epoch: [9][42/78]\tTime  0.083 ( 0.082)\tData  0.064 ( 0.063)\tLoss 0.5521 (0.4777)\n",
      "Epoch: [9][49/78]\tTime  0.080 ( 0.082)\tData  0.062 ( 0.064)\tLoss 0.4023 (0.4774)\n",
      "Epoch: [9][56/78]\tTime  0.080 ( 0.082)\tData  0.061 ( 0.064)\tLoss 0.5423 (0.4785)\n",
      "Epoch: [9][63/78]\tTime  0.083 ( 0.082)\tData  0.064 ( 0.064)\tLoss 0.4952 (0.4769)\n",
      "Epoch: [9][70/78]\tTime  0.080 ( 0.082)\tData  0.063 ( 0.064)\tLoss 0.4855 (0.4741)\n",
      "Epoch: [9][77/78]\tTime  0.040 ( 0.082)\tData  0.030 ( 0.063)\tLoss 0.4996 (0.4737)\n",
      "Epoch: [10][ 0/78]\tTime  0.081 ( 0.081)\tData  0.063 ( 0.063)\tLoss 0.3533 (0.3533)\n",
      "Epoch: [10][ 7/78]\tTime  0.081 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.3247 (0.4759)\n",
      "Epoch: [10][14/78]\tTime  0.082 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.4641 (0.4609)\n",
      "Epoch: [10][21/78]\tTime  0.081 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.4683 (0.4482)\n",
      "Epoch: [10][28/78]\tTime  0.083 ( 0.082)\tData  0.064 ( 0.062)\tLoss 0.4292 (0.4384)\n",
      "Epoch: [10][35/78]\tTime  0.077 ( 0.081)\tData  0.060 ( 0.062)\tLoss 0.3618 (0.4415)\n",
      "Epoch: [10][42/78]\tTime  0.081 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.3076 (0.4420)\n",
      "Epoch: [10][49/78]\tTime  0.081 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.4345 (0.4449)\n",
      "Epoch: [10][56/78]\tTime  0.083 ( 0.081)\tData  0.063 ( 0.062)\tLoss 0.4090 (0.4442)\n",
      "Epoch: [10][63/78]\tTime  0.081 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.5923 (0.4463)\n",
      "Epoch: [10][70/78]\tTime  0.079 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.5148 (0.4480)\n",
      "Epoch: [10][77/78]\tTime  0.043 ( 0.081)\tData  0.032 ( 0.062)\tLoss 0.5944 (0.4525)\n",
      "Epoch: [11][ 0/78]\tTime  0.081 ( 0.081)\tData  0.061 ( 0.061)\tLoss 0.5067 (0.5067)\n",
      "Epoch: [11][ 7/78]\tTime  0.082 ( 0.082)\tData  0.063 ( 0.062)\tLoss 0.3807 (0.4646)\n",
      "Epoch: [11][14/78]\tTime  0.084 ( 0.082)\tData  0.064 ( 0.062)\tLoss 0.5432 (0.4790)\n",
      "Epoch: [11][21/78]\tTime  0.085 ( 0.082)\tData  0.065 ( 0.062)\tLoss 0.4629 (0.4611)\n",
      "Epoch: [11][28/78]\tTime  0.079 ( 0.082)\tData  0.062 ( 0.063)\tLoss 0.3566 (0.4591)\n",
      "Epoch: [11][35/78]\tTime  0.081 ( 0.082)\tData  0.061 ( 0.062)\tLoss 0.4939 (0.4568)\n",
      "Epoch: [11][42/78]\tTime  0.081 ( 0.082)\tData  0.061 ( 0.062)\tLoss 0.4884 (0.4580)\n",
      "Epoch: [11][49/78]\tTime  0.086 ( 0.082)\tData  0.067 ( 0.062)\tLoss 0.3329 (0.4495)\n",
      "Epoch: [11][56/78]\tTime  0.084 ( 0.082)\tData  0.064 ( 0.063)\tLoss 0.5242 (0.4432)\n",
      "Epoch: [11][63/78]\tTime  0.080 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.4486 (0.4466)\n",
      "Epoch: [11][70/78]\tTime  0.080 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.5143 (0.4439)\n",
      "Epoch: [11][77/78]\tTime  0.044 ( 0.082)\tData  0.032 ( 0.063)\tLoss 0.7157 (0.4414)\n",
      "Epoch: [12][ 0/78]\tTime  0.083 ( 0.083)\tData  0.063 ( 0.063)\tLoss 0.3492 (0.3492)\n",
      "Epoch: [12][ 7/78]\tTime  0.083 ( 0.083)\tData  0.064 ( 0.063)\tLoss 0.4678 (0.3883)\n",
      "Epoch: [12][14/78]\tTime  0.080 ( 0.082)\tData  0.062 ( 0.062)\tLoss 0.4245 (0.4065)\n",
      "Epoch: [12][21/78]\tTime  0.079 ( 0.082)\tData  0.061 ( 0.063)\tLoss 0.3721 (0.4146)\n",
      "Epoch: [12][28/78]\tTime  0.077 ( 0.081)\tData  0.060 ( 0.062)\tLoss 0.3774 (0.4174)\n",
      "Epoch: [12][35/78]\tTime  0.084 ( 0.081)\tData  0.065 ( 0.062)\tLoss 0.3637 (0.4297)\n",
      "Epoch: [12][42/78]\tTime  0.083 ( 0.082)\tData  0.064 ( 0.062)\tLoss 0.5097 (0.4260)\n",
      "Epoch: [12][49/78]\tTime  0.085 ( 0.082)\tData  0.066 ( 0.062)\tLoss 0.3834 (0.4278)\n",
      "Epoch: [12][56/78]\tTime  0.083 ( 0.082)\tData  0.063 ( 0.062)\tLoss 0.4728 (0.4277)\n",
      "Epoch: [12][63/78]\tTime  0.079 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.3913 (0.4253)\n",
      "Epoch: [12][70/78]\tTime  0.081 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.3954 (0.4232)\n",
      "Epoch: [12][77/78]\tTime  0.043 ( 0.081)\tData  0.032 ( 0.062)\tLoss 0.5890 (0.4261)\n",
      "Epoch: [13][ 0/78]\tTime  0.082 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.2820 (0.2820)\n",
      "Epoch: [13][ 7/78]\tTime  0.082 ( 0.082)\tData  0.062 ( 0.062)\tLoss 0.3981 (0.4263)\n",
      "Epoch: [13][14/78]\tTime  0.083 ( 0.082)\tData  0.063 ( 0.062)\tLoss 0.4232 (0.4370)\n",
      "Epoch: [13][21/78]\tTime  0.079 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.4365 (0.4422)\n",
      "Epoch: [13][28/78]\tTime  0.082 ( 0.081)\tData  0.063 ( 0.062)\tLoss 0.2068 (0.4268)\n",
      "Epoch: [13][35/78]\tTime  0.080 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.5014 (0.4242)\n",
      "Epoch: [13][42/78]\tTime  0.079 ( 0.081)\tData  0.059 ( 0.062)\tLoss 0.4418 (0.4226)\n",
      "Epoch: [13][49/78]\tTime  0.081 ( 0.081)\tData  0.062 ( 0.062)\tLoss 0.3371 (0.4139)\n",
      "Epoch: [13][56/78]\tTime  0.082 ( 0.081)\tData  0.063 ( 0.062)\tLoss 0.3520 (0.4094)\n",
      "Epoch: [13][63/78]\tTime  0.080 ( 0.081)\tData  0.060 ( 0.062)\tLoss 0.3397 (0.4123)\n",
      "Epoch: [13][70/78]\tTime  0.083 ( 0.082)\tData  0.064 ( 0.062)\tLoss 0.4172 (0.4118)\n",
      "Epoch: [13][77/78]\tTime  0.043 ( 0.081)\tData  0.031 ( 0.062)\tLoss 0.3920 (0.4130)\n",
      "Epoch: [14][ 0/78]\tTime  0.082 ( 0.082)\tData  0.062 ( 0.062)\tLoss 0.5032 (0.5032)\n",
      "Epoch: [14][ 7/78]\tTime  0.081 ( 0.083)\tData  0.062 ( 0.064)\tLoss 0.4056 (0.4154)\n",
      "Epoch: [14][14/78]\tTime  0.080 ( 0.082)\tData  0.062 ( 0.063)\tLoss 0.2892 (0.3938)\n",
      "Epoch: [14][21/78]\tTime  0.078 ( 0.082)\tData  0.061 ( 0.063)\tLoss 0.4509 (0.4170)\n",
      "Epoch: [14][28/78]\tTime  0.080 ( 0.082)\tData  0.062 ( 0.063)\tLoss 0.3551 (0.4111)\n",
      "Epoch: [14][35/78]\tTime  0.079 ( 0.081)\tData  0.060 ( 0.063)\tLoss 0.2149 (0.4056)\n",
      "Epoch: [14][42/78]\tTime  0.081 ( 0.081)\tData  0.063 ( 0.063)\tLoss 0.3364 (0.4091)\n",
      "Epoch: [14][49/78]\tTime  0.081 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.4664 (0.4074)\n",
      "Epoch: [14][56/78]\tTime  0.080 ( 0.081)\tData  0.063 ( 0.062)\tLoss 0.3984 (0.4137)\n",
      "Epoch: [14][63/78]\tTime  0.082 ( 0.081)\tData  0.063 ( 0.062)\tLoss 0.3186 (0.4117)\n",
      "Epoch: [14][70/78]\tTime  0.078 ( 0.081)\tData  0.059 ( 0.063)\tLoss 0.3132 (0.4080)\n",
      "Epoch: [14][77/78]\tTime  0.043 ( 0.081)\tData  0.032 ( 0.062)\tLoss 0.3926 (0.4095)\n",
      "Epoch: [15][ 0/78]\tTime  0.080 ( 0.080)\tData  0.062 ( 0.062)\tLoss 0.2792 (0.2792)\n",
      "Epoch: [15][ 7/78]\tTime  0.083 ( 0.081)\tData  0.064 ( 0.062)\tLoss 0.4172 (0.4144)\n",
      "Epoch: [15][14/78]\tTime  0.077 ( 0.081)\tData  0.060 ( 0.063)\tLoss 0.3976 (0.4137)\n",
      "Epoch: [15][21/78]\tTime  0.080 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.4957 (0.4086)\n",
      "Epoch: [15][28/78]\tTime  0.080 ( 0.081)\tData  0.061 ( 0.063)\tLoss 0.3429 (0.4109)\n",
      "Epoch: [15][35/78]\tTime  0.084 ( 0.081)\tData  0.064 ( 0.063)\tLoss 0.3337 (0.4018)\n",
      "Epoch: [15][42/78]\tTime  0.085 ( 0.082)\tData  0.065 ( 0.063)\tLoss 0.4426 (0.3998)\n",
      "Epoch: [15][49/78]\tTime  0.080 ( 0.082)\tData  0.063 ( 0.064)\tLoss 0.4783 (0.3985)\n",
      "Epoch: [15][56/78]\tTime  0.079 ( 0.082)\tData  0.060 ( 0.063)\tLoss 0.3532 (0.3909)\n",
      "Epoch: [15][63/78]\tTime  0.081 ( 0.082)\tData  0.062 ( 0.063)\tLoss 0.4708 (0.3942)\n",
      "Epoch: [15][70/78]\tTime  0.082 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.4832 (0.3962)\n",
      "Epoch: [15][77/78]\tTime  0.042 ( 0.081)\tData  0.030 ( 0.063)\tLoss 0.3696 (0.3979)\n",
      "Epoch: [16][ 0/78]\tTime  0.082 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.4511 (0.4511)\n",
      "Epoch: [16][ 7/78]\tTime  0.080 ( 0.080)\tData  0.063 ( 0.062)\tLoss 0.3417 (0.3995)\n",
      "Epoch: [16][14/78]\tTime  0.078 ( 0.080)\tData  0.061 ( 0.062)\tLoss 0.3397 (0.3802)\n",
      "Epoch: [16][21/78]\tTime  0.078 ( 0.081)\tData  0.059 ( 0.063)\tLoss 0.2101 (0.3687)\n",
      "Epoch: [16][28/78]\tTime  0.081 ( 0.081)\tData  0.062 ( 0.063)\tLoss 0.3372 (0.3713)\n",
      "Epoch: [16][35/78]\tTime  0.082 ( 0.081)\tData  0.064 ( 0.063)\tLoss 0.2770 (0.3745)\n",
      "Epoch: [16][42/78]\tTime  0.079 ( 0.081)\tData  0.062 ( 0.063)\tLoss 0.4073 (0.3821)\n",
      "Epoch: [16][49/78]\tTime  0.077 ( 0.081)\tData  0.060 ( 0.062)\tLoss 0.4660 (0.3870)\n",
      "Epoch: [16][56/78]\tTime  0.081 ( 0.081)\tData  0.061 ( 0.063)\tLoss 0.2721 (0.3816)\n",
      "Epoch: [16][63/78]\tTime  0.082 ( 0.081)\tData  0.064 ( 0.062)\tLoss 0.5061 (0.3834)\n",
      "Epoch: [16][70/78]\tTime  0.080 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.3626 (0.3817)\n",
      "Epoch: [16][77/78]\tTime  0.043 ( 0.081)\tData  0.032 ( 0.062)\tLoss 0.3929 (0.3840)\n",
      "Epoch: [17][ 0/78]\tTime  0.083 ( 0.083)\tData  0.064 ( 0.064)\tLoss 0.2802 (0.2802)\n",
      "Epoch: [17][ 7/78]\tTime  0.080 ( 0.080)\tData  0.063 ( 0.062)\tLoss 0.4117 (0.3720)\n",
      "Epoch: [17][14/78]\tTime  0.085 ( 0.080)\tData  0.065 ( 0.062)\tLoss 0.3588 (0.3917)\n",
      "Epoch: [17][21/78]\tTime  0.082 ( 0.081)\tData  0.063 ( 0.063)\tLoss 0.3349 (0.3840)\n",
      "Epoch: [17][28/78]\tTime  0.083 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.4166 (0.3813)\n",
      "Epoch: [17][35/78]\tTime  0.084 ( 0.082)\tData  0.064 ( 0.063)\tLoss 0.2595 (0.3792)\n",
      "Epoch: [17][42/78]\tTime  0.080 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.3068 (0.3798)\n",
      "Epoch: [17][49/78]\tTime  0.082 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.3848 (0.3780)\n",
      "Epoch: [17][56/78]\tTime  0.083 ( 0.081)\tData  0.064 ( 0.063)\tLoss 0.4445 (0.3798)\n",
      "Epoch: [17][63/78]\tTime  0.090 ( 0.082)\tData  0.071 ( 0.063)\tLoss 0.4033 (0.3788)\n",
      "Epoch: [17][70/78]\tTime  0.083 ( 0.082)\tData  0.063 ( 0.063)\tLoss 0.4950 (0.3769)\n",
      "Epoch: [17][77/78]\tTime  0.042 ( 0.081)\tData  0.031 ( 0.062)\tLoss 0.6783 (0.3770)\n",
      "Epoch: [18][ 0/78]\tTime  0.080 ( 0.080)\tData  0.063 ( 0.063)\tLoss 0.4201 (0.4201)\n",
      "Epoch: [18][ 7/78]\tTime  0.079 ( 0.080)\tData  0.060 ( 0.062)\tLoss 0.3825 (0.3405)\n",
      "Epoch: [18][14/78]\tTime  0.081 ( 0.080)\tData  0.062 ( 0.062)\tLoss 0.3981 (0.3522)\n",
      "Epoch: [18][21/78]\tTime  0.079 ( 0.081)\tData  0.059 ( 0.062)\tLoss 0.3367 (0.3717)\n",
      "Epoch: [18][28/78]\tTime  0.083 ( 0.081)\tData  0.063 ( 0.062)\tLoss 0.2591 (0.3620)\n",
      "Epoch: [18][35/78]\tTime  0.083 ( 0.081)\tData  0.065 ( 0.062)\tLoss 0.4015 (0.3568)\n",
      "Epoch: [18][42/78]\tTime  0.084 ( 0.081)\tData  0.067 ( 0.062)\tLoss 0.4867 (0.3650)\n",
      "Epoch: [18][49/78]\tTime  0.080 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.3632 (0.3681)\n",
      "Epoch: [18][56/78]\tTime  0.083 ( 0.081)\tData  0.063 ( 0.062)\tLoss 0.3580 (0.3599)\n",
      "Epoch: [18][63/78]\tTime  0.084 ( 0.082)\tData  0.065 ( 0.063)\tLoss 0.3007 (0.3636)\n",
      "Epoch: [18][70/78]\tTime  0.087 ( 0.082)\tData  0.068 ( 0.063)\tLoss 0.3936 (0.3613)\n",
      "Epoch: [18][77/78]\tTime  0.046 ( 0.081)\tData  0.034 ( 0.062)\tLoss 0.4087 (0.3575)\n",
      "Epoch: [19][ 0/78]\tTime  0.079 ( 0.079)\tData  0.059 ( 0.059)\tLoss 0.2532 (0.2532)\n",
      "Epoch: [19][ 7/78]\tTime  0.079 ( 0.084)\tData  0.060 ( 0.062)\tLoss 0.3006 (0.3573)\n",
      "Epoch: [19][14/78]\tTime  0.081 ( 0.083)\tData  0.062 ( 0.063)\tLoss 0.3763 (0.3630)\n",
      "Epoch: [19][21/78]\tTime  0.079 ( 0.083)\tData  0.060 ( 0.062)\tLoss 0.3469 (0.3516)\n",
      "Epoch: [19][28/78]\tTime  0.082 ( 0.082)\tData  0.062 ( 0.062)\tLoss 0.4166 (0.3578)\n",
      "Epoch: [19][35/78]\tTime  0.081 ( 0.082)\tData  0.063 ( 0.062)\tLoss 0.3836 (0.3562)\n",
      "Epoch: [19][42/78]\tTime  0.080 ( 0.082)\tData  0.062 ( 0.062)\tLoss 0.4153 (0.3651)\n",
      "Epoch: [19][49/78]\tTime  0.082 ( 0.082)\tData  0.062 ( 0.062)\tLoss 0.3894 (0.3658)\n",
      "Epoch: [19][56/78]\tTime  0.082 ( 0.082)\tData  0.064 ( 0.062)\tLoss 0.3464 (0.3606)\n",
      "Epoch: [19][63/78]\tTime  0.080 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.5233 (0.3667)\n",
      "Epoch: [19][70/78]\tTime  0.078 ( 0.081)\tData  0.061 ( 0.062)\tLoss 0.3650 (0.3639)\n",
      "Epoch: [19][77/78]\tTime  0.043 ( 0.081)\tData  0.033 ( 0.062)\tLoss 0.2725 (0.3605)\n",
      "Epoch: [20][ 0/78]\tTime  0.083 ( 0.083)\tData  0.064 ( 0.064)\tLoss 0.3410 (0.3410)\n",
      "Epoch: [20][ 7/78]\tTime  0.083 ( 0.081)\tData  0.065 ( 0.062)\tLoss 0.3744 (0.3932)\n",
      "Epoch: [20][14/78]\tTime  0.083 ( 0.081)\tData  0.063 ( 0.062)\tLoss 0.3945 (0.3797)\n",
      "Epoch: [20][21/78]\tTime  0.083 ( 0.081)\tData  0.063 ( 0.062)\tLoss 0.3570 (0.3734)\n",
      "Epoch: [20][28/78]\tTime  0.078 ( 0.082)\tData  0.061 ( 0.062)\tLoss 0.3374 (0.3623)\n",
      "Epoch: [20][35/78]\tTime  0.079 ( 0.081)\tData  0.061 ( 0.063)\tLoss 0.2206 (0.3590)\n",
      "Epoch: [20][42/78]\tTime  0.080 ( 0.082)\tData  0.061 ( 0.063)\tLoss 0.3596 (0.3515)\n",
      "Epoch: [20][49/78]\tTime  0.084 ( 0.082)\tData  0.065 ( 0.063)\tLoss 0.3465 (0.3492)\n",
      "Epoch: [20][56/78]\tTime  0.085 ( 0.082)\tData  0.065 ( 0.063)\tLoss 0.2573 (0.3448)\n",
      "Epoch: [20][63/78]\tTime  0.081 ( 0.082)\tData  0.064 ( 0.063)\tLoss 0.3857 (0.3460)\n",
      "Epoch: [20][70/78]\tTime  0.079 ( 0.082)\tData  0.061 ( 0.063)\tLoss 0.3930 (0.3461)\n",
      "Epoch: [20][77/78]\tTime  0.043 ( 0.081)\tData  0.032 ( 0.063)\tLoss 0.3989 (0.3454)\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLP(dim=2304+9).cuda()\n",
    "# weights = [1., 1., 0.1, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "# class_weights = torch.FloatTensor(weights)\n",
    "# sup_criterion = nn.CrossEntropyLoss(weight=class_weights).cuda()\n",
    "sup_criterion = nn.CrossEntropyLoss().cuda()\n",
    "sup_opt = optim.Adam(mlp_classifier.parameters(), lr=1e-3)\n",
    "def grad_model(model, requires_grad: bool):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad\n",
    "\n",
    "# freeze representation, use encoder to get features, then train the FCN\n",
    "def cls_train(train_loader, model, cls_mlp, criterion, optimizer, epoch):\n",
    "    enc_model = model.encoder\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # switch to train mode\n",
    "    enc_model.train()\n",
    "    grad_model(enc_model, False)\n",
    "    cls_mlp.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # if args.gpu is not None:\n",
    "        #     images[0] = images[0].cuda(args.gpu, non_blocking=True)\n",
    "        #     images[1] = images[1].cuda(args.gpu, non_blocking=True)\n",
    "\n",
    "        # compute output and loss\n",
    "        # print(len(images))\n",
    "        # input()\n",
    "        images = batch['obs'].cuda()\n",
    "        prev_action = batch['prev_a'].cuda()\n",
    "        target = batch['a']\n",
    "        z = enc_model(images.cuda())\n",
    "        # print(p1.size(), p2.size(), z1.size(), z2.size())\n",
    "        z = torch.cat((z, prev_action), dim=-1)\n",
    "        logits = cls_mlp(z)\n",
    "        loss = criterion(logits, target.cuda().flatten())\n",
    "\n",
    "        losses.update(loss.item(), images[0].size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % (max(len(train_loader)//10, 1)) == 0:\n",
    "            progress.display(i)\n",
    "for epoch in range(1, 1 + 20):\n",
    "    cls_train(cls_train_dataloader,\n",
    "              model,\n",
    "              mlp_classifier,\n",
    "              sup_criterion, sup_opt, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'encoder_dict': model.encoder.state_dict(),\n",
    "    'mlp_dict': mlp_classifier.state_dict()\n",
    "}, 'test_simsiam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\n"
     ]
    }
   ],
   "source": [
    "!python simsiam_eval.py --env-name MiniWorld-OneRoom-v0 --model_path test_simsiam.pt"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e2e8246255a260daa8af80185d2c5cff5556d2b33472f78e1dcb326d52dc230"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('irl_demo': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
